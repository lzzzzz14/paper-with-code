# Autoregressive Visual Tracking（ARTrack）

### 传统目标跟踪

将跟踪是为每帧模板匹配问题（忽略了视频帧时间的时序依赖性）

### ARTrack

<mark>将跟踪视为坐标序列解释任务</mark>

​	通过学习一个简单的端到端模型来进行直接轨迹估计

可以建模轨迹的时序演变，以保持跟踪结果的连贯性

* 更好的处理目标变形、尺度变化、遮挡和干扰
* 不需要定制的定位头和后处理

## Tracking as Sequence Interpretation

把视觉跟踪作为一个连续的坐标解释任务，以条件概率的形式表述：
$$
P(Y^t∣Y^{t−N:t−1},(C,Z,X^t))
$$

* Z 和 X<sub>t</sub> 是时间步 t 的给定模板和搜索图像
* C是命令标记
* Y 表示与 X 关联的目标序列

（模板 Z 可以通过更新机制在每个时间步长进行更新，或者干脆作为初始模板）

（N 表示在视觉跟踪中考虑了多少个过去的时间步来帮助预测目标的位置或状态，如果 N=0，我们只会考虑当前时间步的信息）

### 由一下主要组件组成

1. **序列构建**：
   - 这部分是关于如何开始跟踪一个物体的。我们有一个视频序列，就像电影一样，以及一个初始的物体框，它是一个矩形区域，用来标识视频中的目标物体的位置。
   - 视觉跟踪器的任务是预测一个连续的矩形框序列，就像一系列相框，这些框会随着时间在视频中移动，以跟踪目标物体。
   - 这些预测的框被映射到一个共同的坐标系统中，这样它们可以方便地在不同帧之间进行比较，然后被转换成一系列的离散标记，这些标记描述了目标物体在不同帧中的位置或状态。
2. **网络架构**：
   - 这部分涉及到一个用来处理图像和预测物体位置的计算机程序的"脑部"。
   - 我们使用了一种叫做编解码器的结构，编码器的作用是将图像转化为一种特殊的表示，这个表示包含了图像中的信息，特别是目标物体的信息。
   - 然后，解码器会解释这个表示，从中预测出目标框的序列，就像是翻译一种语言成为另一种语言一样。
3. **目标函数**：
   - 这是关于如何训练这个视觉跟踪系统的一部分。
   - 我们将这个系统放在训练用的视频帧上，就像教一个机器学习模型一样。
   - 我们使用一个叫做"结构化的损失函数"的工具来告诉模型如何调整自己的参数，以便使它的预测与实际目标框的序列尽可能一致。
   - 此外，我们还可以探索一些特定的任务要求，以进一步提高跟踪性能，比如针对某种特定类型的目标进行优化。

## Sequence Construction from Object Trajectory

### Tokenization

对连续坐标进行离散化，以避免描述连续坐标所需的大量参数，这称为标记化。

（比如：将小数转换为离散的整数标记）

### Trajectory coordinate mapping（坐标映射）

表示物体的坐标通常是在搜索区域（相对于整张图片进行裁剪），需要将不同帧的框映射到一个坐标系中

将前面 N 帧的盒子坐标缓存在全局坐标系中，并在搜索区域被裁剪后将其映射到当前坐标系

### Representation range of vocabulary

1. **词汇表的表示范围**：在视觉跟踪中，词汇表通常用来表示物体的位置或状态，而这个表示范围可以根据搜索区域的大小进行设置。搜索区域是一个用于限定目标物体可能出现的区域。
2. **问题的挑战**：然而，在实际情况中，物体可能会移动得非常快，以至于前面的轨迹序列有时可能会超出搜索区域的边界。这可能导致模型无法捕获物体的完整运动信息，因为部分信息位于搜索区域之外。
3. **解决方法**：为了解决这个问题，作者提出了一个方法，即将词汇表的表示范围扩展为搜索区域范围的倍数。例如，如果搜索区域的范围是[0.0, 1.0]，那么词汇表的表示范围会被扩展为[-0.5, 1.5]。这样做的好处是词汇表可以包含位于搜索区域之外的坐标或位置信息。
4. **为何这样做**：这个扩展的表示范围允许模型捕获更多关于物体运动的线索，即使物体的位置可能超出搜索区域的边界。这可以帮助模型更准确地跟踪和预测目标的位置，尤其是在物体移动速度较快的情况下。

## Network Architecture

### Encoder

这里使用的是VIT编码器

​	模板和搜索图像首先被分割成小块，被平面化和投影以生成一系列的标记嵌入

​	将模板和搜索标记加上位置和身份嵌入，串联起来。然后送入vit骨干网络

### Decoder

使用Transformer解码器生成目标序列

​	以前面的坐标标记、命令标记和视觉特征为条件，逐步解码整个序列

​	命令标记（C）提供一个轨迹建议，将模板（Z）和搜索（X<sub>t</sub>）相匹配，来获得更准确的坐标预测（Y<sub>t</sub>）

解码器需要两种注意力

​	自注意力（带有因果掩码），在坐标标记之间执行以传达时空信息

​	交叉注意力，将运动线索与视觉线索相结合以进行最终预测

![image-20230604152117203](https://img-blog.csdnimg.cn/img_convert/56ee238e07925ef8d7273be1044b5dc0.png)

## Training and Inference

### Training

1. **训练目标**：ARTrack的目标是在视觉跟踪任务中，保持跨视频帧的目标定位精度。它采用了一个结构化的目标，即通过最大化标记序列的对数可能性来学习模型。这个目标函数的形式是在每个时间步 t 下，最大化预测的目标序列 Y^t 与之前 N 个时间步的目标序列 $Y^{t-N:t-1}$ 以及其他条件信息 ($C, Z, X^t$) 的条件概率的对数。
2. **损失函数**：模型的训练使用了一个结构化损失函数，通常使用 softmax 交叉熵损失函数。这个损失函数的目标是使模型的预测尽可能接近实际的目标序列，从 t = 1 到 T 的所有时间步都要最大化这个对数可能性。

$$
maximize\sum_{t=1}^{T} \log P(Y^t | Y^{t-N:t-1}, (C, Z, X^t))
$$

3. **时间步和初始化**：在开始训练时，对于 t ≤ N 的时间步，模型会使用已缓存的时空提示（$Y^{t-N:t-1}$），并且初始的（$Y_1$）会填充序列。然后，模型会逐渐使用新的预测来更新序列。

4. **引入 SIoU 损失**：尽管结构化损失对于训练模型有效，但它忽略了令牌的物理属性，例如坐标的空间关系。为了更好地考虑预测边界框和实际边界框之间的空间相关性，作者引入了 SIoU 损失（Spatial Intersection over Union）。SIoU 损失有助于更准确地衡量两个边界框之间的空间重叠。

5. **整体损失函数**：最终的损失函数由两部分组成，一部分是交叉熵损失($L_ce$)，另一部分是 SIoU 损失($L_SIou$)。它们之间通过一个权重参数 λ 来平衡，λ 的值决定了这两个损失的相对重要性。

$$
L = L_{ce} + \lambda L_{SIoU}
$$

### Inference


在推理过程中，模型用于预测目标的位置。为了选择最有可能的目标位置，我们使用了一种称为 "argmax 抽样法" 的方法，它从模型的可能性分布中选择概率最高的结果。这个可能性分布是根据条件概率 $P(Y^t|Y^{t-N:t-1},(C,Z,X^t)) $计算得到的。

这里有几个要点：

1. **argmax 抽样**：argmax 抽样是一种选择概率最大的策略，它有助于找到在给定条件下最有可能的目标位置。这意味着我们选择具有最高概率的目标位置作为最终预测。
2. **不需要 EOS 令牌**：一些序列任务可能需要一个特殊的结束标记（EOS 令牌）来指示序列的结束。但在这个问题中，序列的长度是固定的，因此不需要额外的 EOS 令牌来结束序列的预测。
3. **去量化**：一旦我们获得了离散标记，就需要将它们转换回连续的坐标。这是因为在训练过程中，我们使用了离散标记来表示位置，但在实际应用中，我们需要得到连续的坐标。
