# 相对位置偏置（relative position bias）

基本形式如下：
$$
Attention(Q,K,V)=Softmax(QK^T+B)V
$$
其中Q，K ∈ R\^n×d，B∈R\^n×n，n是token vector的数目

B的作用是给attention map QK^^T的每个元素加了一个值，其本质就是希望attention map进一步有所偏重

attention map中某个值越低，经过softmax之后，该值会更低，对最终特征的贡献就低