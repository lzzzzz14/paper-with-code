![image-20230917162851557](C:\Users\12570\AppData\Roaming\Typora\typora-user-images\image-20230917162851557.png)

整个框架是一个单流的框架

1. 将模板和搜索区域，分割，展平，线性投影，让如vit的encoder模块中
2. 在vit的encoder模块中，在多头注意力后面加入了early candidate elimination模块



* 输入（一对图像）

模板图像块z（尺寸：3×H~z~×W~z~），搜索区域块x（3×H~x~×W~x~）

* 被分割、展平成一些列patch

z~p~（N~z~×3×P^2^）：P×P是每块的分辨率，N~z~=H~z~×W~z~÷P^2^

z~x~（N~x~×3×P^2^）：P×P是每块的分辨率，N~x~=H~x~×W~x~÷P^2^

* linear projection layer with parameter E（实际就是过1×1卷积）

$$
H_z^0=[z^1_pE;z^2_pE;...z^{N_z}_pE]+P_z, \quad \quad \quad \quad \quad \quad \quad  E\in R^{(3 \cdot P^2)×D},\quad P_z\in R^(N_z × D)
$$

$$
H_x^0=[x^1_pE;x^2_pE;...x^{N_x}_pE]+P_x,\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad  P_x\in R^{N_x×D}
$$

* Position embeddings

P~z~ and P~x~ ：分别加到每个patch上

* token embeddings之后的形状

H~z~（N~z~×D×P^2^）：就是N~z~个patch，每个patch的形状是（D×P×P）

H~x~（N~x~×D×P^2^）：就是N~x~个patch，每个patch的形状是（D×P×P）

* 作者进行了消融实验，是否要加一些标识来区分template和search region的patch，实验证明，加入标识并没有显著提升性能，所以省略了这些嵌入

* 然后将这些token concat起来，变味了H~zx~

H~zx~（(N~z~+N~x~)×D×P^2^)：就是有N~z~+N~x~个patch，每个patch的形状是（D×P×P）



